---
title: Preferential Temporal Difference Learning
abstract: Temporal-Difference (TD) learning is a general and very useful tool for
  estimating the value function of a given policy, which in turn is required to find
  good policies. Generally speaking, TD learning updates states whenever they are
  visited. When the agent lands in a state, its value can be used to compute the TD-error,
  which is then propagated to other states. However, it may be interesting, when computing
  updates, to take into account other information than whether a state is visited
  or not. For example, some states might be more important than others (such as states
  which are frequently seen in a successful trajectory). Or, some states might have
  unreliable value estimates (for example, due to partial observability or lack of
  data), making their values less desirable as targets. We propose an approach to
  re-weighting states used in TD updates, both when they are the input and when they
  provide the target for the update. We prove that our approach converges with linear
  function approximation and illustrate its desirable empirical behaviour compared
  to other TD-style methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: anand21a
month: 0
tex_title: Preferential Temporal Difference Learning
firstpage: 286
lastpage: 296
page: 286-296
order: 286
cycles: false
bibtex_author: Anand, Nishanth and Precup, Doina
author:
- given: Nishanth
  family: Anand
- given: Doina
  family: Precup
date: 2021-07-01
address:
container-title: Proceedings of the 38th International Conference on Machine Learning
volume: '139'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 7
  - 1
pdf: http://proceedings.mlr.press/v139/anand21a/anand21a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v139/anand21a/anand21a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
